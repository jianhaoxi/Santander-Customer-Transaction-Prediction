{"cells":[{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Santander-Customer-Transaction-Prediction</font>**"},{"metadata":{},"cell_type":"markdown","source":"This is my second kaggle competition completed, where I am supposed to predict whether a Santander customer will perform a specific transaction or not.\nIn this kernel, I am recording how the work is done."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport lightgbm as lgb\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Loading data</font>**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain_copy = train.copy()\ntest_copy = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Data exploration</font>**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are 200 features without their real world name and 200k rows of customer record in both training and testing data, also columns of customer ID and target in training set which means whether they did the transaction.\nAnd there are 200k rows in test data too."},{"metadata":{"trusted":true},"cell_type":"code","source":"if (train_copy.isnull().sum().sum() == 0):\n    print('There is no missing value in training set')\nelse:\n    print('There are ' + str(train_copy.isnull().sum().sum()) + ' missing values in training set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (test_copy.isnull().sum().sum() == 0):\n    print('There is no missing value in testing set')\nelse:\n    print('There are ' + str(train_copy.isnull().sum().sum()) + ' missing values in testing set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! There is no missing value in both training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown, training data and testing data are quite similar in each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_copy['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there exsists imbalanced data issue with only 10% records responding 1."},{"metadata":{},"cell_type":"markdown","source":"To handle this problem, one can just cut '0' responding data to around numbers of positive records using 'sample()' command, but he will loss lots of information, or apply resampling mathod to create positive records.\nAs a result, resampling does not improve the auc much while the former way does improve but not good enough."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_copy['target'].values\ntrain_X_column_name = train_copy.drop(['target', 'ID_code'], axis=1).columns\ntrain_X = train_copy.drop(['target', 'ID_code'], axis=1).values\ntest_X = test_copy.drop(['ID_code'], axis=1).values\ntrain_X_copy = train_X.copy()\ntest_X_copy = test_X.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Until now, I have separated the training data into features and targets, and picked the features from testing data."},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Preprocessing data</font>**"},{"metadata":{},"cell_type":"markdown","source":"Thanks to Felipe Mello's idea that [detecting fake data](http://www.kaggle.com/felipemello/step-by-step-guide-to-the-magic-lb-0-922) in test set, the auc score of my results improved a lot.\n\nSo, the basic ideas are:\n1. There exsists fake data in test dataset\n2. The unique values in each column of test set are important"},{"metadata":{},"cell_type":"markdown","source":"To get the real samples, I will look for samples that have an unique value at least in one feature among all 200k records, because it indicates that no other samples copyed from this sample. And it is obvious that if all features of a sample are not unique, this sample is probably sythesized from others."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_samples = []\nunique_count = np.zeros_like(test_X)\nfor feature in range(test_X.shape[1]):\n    _, index_, count_ = np.unique(test_X[:, feature], return_index=True, return_counts=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\nreal_sample_index = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_sample_index = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\ntest_X_real = test_X[real_sample_index].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we show how much real and synthetic data are in the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are ' + str(len(real_sample_index)) + ' real data samples in test set')\nprint('There are ' + str(len(synthetic_sample_index)) + ' synthetic data samples in test set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which is reasonable."},{"metadata":{},"cell_type":"markdown","source":"Because there are exactly 100k real test data and 100k synthetic data, for each synthetic sample, I will capture those features that have only one instance in the real samples set with the same value, this instance has to be one of the samples' generators."},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_for_each_synthetic_sample = []\nfor cur_sample_index in synthetic_sample_index[:20000]:\n    cur_synthetic_sample = test_X[cur_sample_index]\n    potential_generators = test_X_real == cur_synthetic_sample\n\n    features_mask = np.sum(potential_generators, axis=0) == 1\n    verified_generators_mask = np.any(potential_generators[:, features_mask], axis=1)\n    verified_generators_for_sample = real_sample_index[np.argwhere(verified_generators_mask)[:, 0]]\n    generator_for_each_synthetic_sample.append(set(verified_generators_for_sample))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then find the Public and Private splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"public_LB = generator_for_each_synthetic_sample[0]\nfor x in generator_for_each_synthetic_sample:\n    if public_LB.intersection(x):\n        public_LB = public_LB.union(x)\n\nprivate_LB = generator_for_each_synthetic_sample[1]\nfor x in generator_for_each_synthetic_sample:\n    if private_LB.intersection(x):\n        private_LB = private_LB.union(x)\n\nprivate_LB = list(private_LB)\npublic_LB = list(public_LB)\nfull = np.concatenate([train_X, np.concatenate([test_X[private_LB], test_X[public_LB]])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print('There are ' + str(len(private_LB)) + ' data samples for public score in real data set')\nprint('There are ' + str(len(public_LB)) + ' data samples for private score in real data set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which is obvious because half of the test data are used for private score and the other half are used for public score.\nSo this technique here is specially for the competition, it may not be helpful in the real world."},{"metadata":{"trusted":true},"cell_type":"code","source":"full = pd.DataFrame(full)\nfull.columns = train_X_column_name\ntrain_X = pd.DataFrame(train_X)\ntrain_X.columns = train_X_column_name\ntest_X = pd.DataFrame(test_X)\ntest_X .columns = train_X_column_name\n\nfor feat in ['var_' + str(x) for x in range(200)]:\n    count_values = full.groupby(feat)[feat].count()\n    train_X['new_' + feat] = count_values.loc[train_X[feat]].values\n    test_X['new_' + feat] = count_values.loc[test_X[feat]].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After putting training data and real testing data without target together, I created new columns for each feature that indicates unique values count.\n\nHere, it is OK to take test data into consideration because test part will not be used when training the model, only training data and their corresponding targets are pluged in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 0\nparam = {\n    'num_leaves': 8,\n    'min_data_in_leaf': 17,\n    'learning_rate': 0.01,\n    'min_sum_hessian_in_leaf': 9.67,\n    'bagging_fraction': 0.8329,\n    'bagging_freq': 2,\n    'feature_fraction': 1,\n    'lambda_l1': 0.6426,\n    'lambda_l2': 0.3067,\n    'min_gain_to_split': 0.02832,\n    'max_depth': -1,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbosity': -1,\n    'metric': 'auc',\n    'is_unbalance': True,\n    'save_binary': True,\n    'boost_from_average': 'false',\n    'num_threads': 8\n}\n\n\niterations = 110\ntest_hat = np.zeros([200000, 200])\ni = 0\nfor feature in ['var_' + str(x) for x in range(200)]:  # loop over all features\n    # print(feature)\n    feat_choices = [feature, 'new_' + feature]\n    lgb_train = lgb.Dataset(train_X[feat_choices], train_y)\n    gbm = lgb.train(param, lgb_train, iterations, verbose_eval=-1)\n    test_hat[:, i] = gbm.predict(test_X[feat_choices], num_iteration=gbm.best_iteration)\n    i += 1\n\nsub_preds = test_hat.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So I used Light Gredient Boosting Machine here. \n\nThe reason is LightGBM is an ensemble method, so it performs better than single algorithm, and 'Light' means it runs faster than grediant boosting.\n\nAnother trick here is I have applied LightGBM columns by columns, which speeded up the process and is proved having a better result."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID_code'] = test_copy['ID_code']\nsub['target'] = sub_preds\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}