{"cells":[{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Santander-Customer-Transaction-Prediction</font>**"},{"metadata":{},"cell_type":"markdown","source":"This is my second kaggle competition completed, where I am supposed to predict whether a Santander customer will perform a specific transaction or not.\nIn this kernel, I will use Convolutional Neural Network."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Conv1D, Flatten\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport seaborn as sns","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Loading data</font>**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain_copy = train.copy()\ntest_copy = test.copy()","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=5>Data exploration</font>**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"  ID_code    var_0    var_1   ...     var_197  var_198  var_199\n0  test_0  11.0656   7.7798   ...     10.7200  15.4722  -8.7197\n1  test_1   8.5304   1.2543   ...      9.8714  19.1293 -20.9760\n2  test_2   5.4827 -10.3581   ...      7.0618  19.8956 -23.1794\n3  test_3   8.5374  -1.3222   ...      9.2295  13.0168  -4.2108\n4  test_4  11.7058  -0.1327   ...      7.2882  13.9260  -9.1846\n\n[5 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>11.0656</td>\n      <td>7.7798</td>\n      <td>12.9536</td>\n      <td>9.4292</td>\n      <td>11.4327</td>\n      <td>-2.3805</td>\n      <td>5.8493</td>\n      <td>18.2675</td>\n      <td>2.1337</td>\n      <td>8.8100</td>\n      <td>-2.0248</td>\n      <td>-4.3554</td>\n      <td>13.9696</td>\n      <td>0.3458</td>\n      <td>7.5408</td>\n      <td>14.5001</td>\n      <td>7.7028</td>\n      <td>-19.0919</td>\n      <td>15.5806</td>\n      <td>16.1763</td>\n      <td>3.7088</td>\n      <td>18.8064</td>\n      <td>1.5899</td>\n      <td>3.0654</td>\n      <td>6.4509</td>\n      <td>14.1192</td>\n      <td>-9.4902</td>\n      <td>-2.1917</td>\n      <td>5.7107</td>\n      <td>3.7864</td>\n      <td>-1.7981</td>\n      <td>9.2645</td>\n      <td>2.0657</td>\n      <td>12.7753</td>\n      <td>11.3334</td>\n      <td>8.1462</td>\n      <td>-0.0610</td>\n      <td>3.5331</td>\n      <td>9.7804</td>\n      <td>...</td>\n      <td>5.9232</td>\n      <td>5.4113</td>\n      <td>3.8302</td>\n      <td>5.7380</td>\n      <td>-8.6105</td>\n      <td>22.9530</td>\n      <td>2.5531</td>\n      <td>-0.2836</td>\n      <td>4.3416</td>\n      <td>5.1855</td>\n      <td>4.2603</td>\n      <td>1.6779</td>\n      <td>29.0849</td>\n      <td>8.4685</td>\n      <td>18.1317</td>\n      <td>12.2818</td>\n      <td>-0.6912</td>\n      <td>10.2226</td>\n      <td>-5.5579</td>\n      <td>2.2926</td>\n      <td>-4.5358</td>\n      <td>10.3903</td>\n      <td>-15.4937</td>\n      <td>3.9697</td>\n      <td>31.3521</td>\n      <td>-1.1651</td>\n      <td>9.2874</td>\n      <td>-23.5705</td>\n      <td>13.2643</td>\n      <td>1.6591</td>\n      <td>-2.1556</td>\n      <td>11.8495</td>\n      <td>-1.4300</td>\n      <td>2.4508</td>\n      <td>13.7112</td>\n      <td>2.4669</td>\n      <td>4.3654</td>\n      <td>10.7200</td>\n      <td>15.4722</td>\n      <td>-8.7197</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>8.5304</td>\n      <td>1.2543</td>\n      <td>11.3047</td>\n      <td>5.1858</td>\n      <td>9.1974</td>\n      <td>-4.0117</td>\n      <td>6.0196</td>\n      <td>18.6316</td>\n      <td>-4.4131</td>\n      <td>5.9739</td>\n      <td>-1.3809</td>\n      <td>-0.3310</td>\n      <td>14.1129</td>\n      <td>2.5667</td>\n      <td>5.4988</td>\n      <td>14.1853</td>\n      <td>7.0196</td>\n      <td>4.6564</td>\n      <td>29.1609</td>\n      <td>0.0910</td>\n      <td>12.1469</td>\n      <td>3.1389</td>\n      <td>5.2578</td>\n      <td>2.4228</td>\n      <td>16.2064</td>\n      <td>13.5023</td>\n      <td>-5.2341</td>\n      <td>-3.6648</td>\n      <td>5.7080</td>\n      <td>2.9965</td>\n      <td>-10.4720</td>\n      <td>11.4938</td>\n      <td>-0.9660</td>\n      <td>15.3445</td>\n      <td>10.6361</td>\n      <td>0.8966</td>\n      <td>6.7428</td>\n      <td>2.3421</td>\n      <td>12.8678</td>\n      <td>...</td>\n      <td>30.9641</td>\n      <td>5.6723</td>\n      <td>3.6873</td>\n      <td>13.0429</td>\n      <td>-10.6572</td>\n      <td>15.5134</td>\n      <td>3.2185</td>\n      <td>9.0535</td>\n      <td>7.0535</td>\n      <td>5.3924</td>\n      <td>-0.7720</td>\n      <td>-8.1783</td>\n      <td>29.9227</td>\n      <td>-5.6274</td>\n      <td>10.5018</td>\n      <td>9.6083</td>\n      <td>-0.4935</td>\n      <td>8.1696</td>\n      <td>-4.3605</td>\n      <td>5.2110</td>\n      <td>0.4087</td>\n      <td>12.0030</td>\n      <td>-10.3812</td>\n      <td>5.8496</td>\n      <td>25.1958</td>\n      <td>-8.8468</td>\n      <td>11.8263</td>\n      <td>-8.7112</td>\n      <td>15.9072</td>\n      <td>0.9812</td>\n      <td>10.6165</td>\n      <td>8.8349</td>\n      <td>0.9403</td>\n      <td>10.1282</td>\n      <td>15.5765</td>\n      <td>0.4773</td>\n      <td>-1.4852</td>\n      <td>9.8714</td>\n      <td>19.1293</td>\n      <td>-20.9760</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>5.4827</td>\n      <td>-10.3581</td>\n      <td>10.1407</td>\n      <td>7.0479</td>\n      <td>10.2628</td>\n      <td>9.8052</td>\n      <td>4.8950</td>\n      <td>20.2537</td>\n      <td>1.5233</td>\n      <td>8.3442</td>\n      <td>-4.7057</td>\n      <td>-3.0422</td>\n      <td>13.6751</td>\n      <td>3.8183</td>\n      <td>10.8535</td>\n      <td>14.2126</td>\n      <td>9.8837</td>\n      <td>2.6541</td>\n      <td>21.2181</td>\n      <td>20.8163</td>\n      <td>12.4666</td>\n      <td>12.3696</td>\n      <td>4.7473</td>\n      <td>2.7936</td>\n      <td>5.2189</td>\n      <td>13.5670</td>\n      <td>-15.4246</td>\n      <td>-0.1655</td>\n      <td>7.2633</td>\n      <td>3.4310</td>\n      <td>-9.1508</td>\n      <td>9.7320</td>\n      <td>3.1062</td>\n      <td>22.3076</td>\n      <td>11.9593</td>\n      <td>9.9255</td>\n      <td>4.0702</td>\n      <td>4.9934</td>\n      <td>8.0667</td>\n      <td>...</td>\n      <td>39.3654</td>\n      <td>5.5228</td>\n      <td>3.3159</td>\n      <td>4.3324</td>\n      <td>-0.5382</td>\n      <td>13.3009</td>\n      <td>3.1243</td>\n      <td>-4.1731</td>\n      <td>1.2330</td>\n      <td>6.1513</td>\n      <td>-0.0391</td>\n      <td>1.4950</td>\n      <td>16.8874</td>\n      <td>-2.9787</td>\n      <td>27.4035</td>\n      <td>15.8819</td>\n      <td>-10.9660</td>\n      <td>15.6415</td>\n      <td>-9.4056</td>\n      <td>4.4611</td>\n      <td>-3.0835</td>\n      <td>8.5549</td>\n      <td>-2.8517</td>\n      <td>13.4770</td>\n      <td>24.4721</td>\n      <td>-3.4824</td>\n      <td>4.9178</td>\n      <td>-2.0720</td>\n      <td>11.5390</td>\n      <td>1.1821</td>\n      <td>-0.7484</td>\n      <td>10.9935</td>\n      <td>1.9803</td>\n      <td>2.1800</td>\n      <td>12.9813</td>\n      <td>2.1281</td>\n      <td>-7.1086</td>\n      <td>7.0618</td>\n      <td>19.8956</td>\n      <td>-23.1794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>8.5374</td>\n      <td>-1.3222</td>\n      <td>12.0220</td>\n      <td>6.5749</td>\n      <td>8.8458</td>\n      <td>3.1744</td>\n      <td>4.9397</td>\n      <td>20.5660</td>\n      <td>3.3755</td>\n      <td>7.4578</td>\n      <td>0.0095</td>\n      <td>-5.0659</td>\n      <td>14.0526</td>\n      <td>13.5010</td>\n      <td>8.7660</td>\n      <td>14.7352</td>\n      <td>10.0383</td>\n      <td>-15.3508</td>\n      <td>2.1273</td>\n      <td>21.4797</td>\n      <td>14.5372</td>\n      <td>12.5527</td>\n      <td>2.9707</td>\n      <td>4.2398</td>\n      <td>13.7796</td>\n      <td>14.1408</td>\n      <td>1.0061</td>\n      <td>-1.3479</td>\n      <td>5.2570</td>\n      <td>6.5911</td>\n      <td>6.2161</td>\n      <td>9.5540</td>\n      <td>2.3628</td>\n      <td>10.2124</td>\n      <td>10.8047</td>\n      <td>-2.5588</td>\n      <td>6.0720</td>\n      <td>3.2613</td>\n      <td>16.5632</td>\n      <td>...</td>\n      <td>19.7251</td>\n      <td>5.3882</td>\n      <td>3.6775</td>\n      <td>7.4753</td>\n      <td>-11.0780</td>\n      <td>24.8712</td>\n      <td>2.6415</td>\n      <td>2.2673</td>\n      <td>7.2788</td>\n      <td>5.6406</td>\n      <td>7.2048</td>\n      <td>3.4504</td>\n      <td>2.4130</td>\n      <td>11.1674</td>\n      <td>14.5499</td>\n      <td>10.6151</td>\n      <td>-5.7922</td>\n      <td>13.9407</td>\n      <td>7.1078</td>\n      <td>1.1019</td>\n      <td>9.4590</td>\n      <td>9.8243</td>\n      <td>5.9917</td>\n      <td>5.1634</td>\n      <td>8.1154</td>\n      <td>3.6638</td>\n      <td>3.3102</td>\n      <td>-19.7819</td>\n      <td>13.4499</td>\n      <td>1.3104</td>\n      <td>9.5702</td>\n      <td>9.0766</td>\n      <td>1.6580</td>\n      <td>3.5813</td>\n      <td>15.1874</td>\n      <td>3.1656</td>\n      <td>3.9567</td>\n      <td>9.2295</td>\n      <td>13.0168</td>\n      <td>-4.2108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>11.7058</td>\n      <td>-0.1327</td>\n      <td>14.1295</td>\n      <td>7.7506</td>\n      <td>9.1035</td>\n      <td>-8.5848</td>\n      <td>6.8595</td>\n      <td>10.6048</td>\n      <td>2.9890</td>\n      <td>7.1437</td>\n      <td>5.1025</td>\n      <td>-3.2827</td>\n      <td>14.1013</td>\n      <td>8.9672</td>\n      <td>4.7276</td>\n      <td>14.5811</td>\n      <td>11.8615</td>\n      <td>3.1480</td>\n      <td>18.0126</td>\n      <td>13.8006</td>\n      <td>1.6026</td>\n      <td>16.3059</td>\n      <td>6.7954</td>\n      <td>3.6015</td>\n      <td>13.6569</td>\n      <td>13.8807</td>\n      <td>8.6228</td>\n      <td>-2.2654</td>\n      <td>5.2255</td>\n      <td>7.0165</td>\n      <td>-15.6961</td>\n      <td>10.6239</td>\n      <td>-4.7674</td>\n      <td>17.5447</td>\n      <td>11.8668</td>\n      <td>3.0154</td>\n      <td>4.2546</td>\n      <td>6.7601</td>\n      <td>5.9613</td>\n      <td>...</td>\n      <td>22.8700</td>\n      <td>5.6688</td>\n      <td>6.1159</td>\n      <td>13.2433</td>\n      <td>-11.9785</td>\n      <td>26.2040</td>\n      <td>3.2348</td>\n      <td>-5.5775</td>\n      <td>5.7036</td>\n      <td>6.1717</td>\n      <td>-1.6039</td>\n      <td>-2.4866</td>\n      <td>17.2728</td>\n      <td>2.3640</td>\n      <td>14.0037</td>\n      <td>12.9165</td>\n      <td>-12.0311</td>\n      <td>10.1161</td>\n      <td>-8.7562</td>\n      <td>6.0889</td>\n      <td>-1.3620</td>\n      <td>10.3559</td>\n      <td>-7.4915</td>\n      <td>9.4588</td>\n      <td>3.9829</td>\n      <td>5.8580</td>\n      <td>8.3635</td>\n      <td>-24.8254</td>\n      <td>11.4928</td>\n      <td>1.6321</td>\n      <td>4.2259</td>\n      <td>9.1723</td>\n      <td>1.2835</td>\n      <td>3.3778</td>\n      <td>19.5542</td>\n      <td>-0.2860</td>\n      <td>-5.1612</td>\n      <td>7.2882</td>\n      <td>13.9260</td>\n      <td>-9.1846</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(200000, 202)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.shape","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(200000, 201)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are 200 features without their real world name and 200k rows of customer record in both training and testing data, also columns of customer ID and target in training set which means whether they did the transaction.\nAnd there are 200k rows in test data too."},{"metadata":{"trusted":true},"cell_type":"code","source":"if (train_copy.isnull().sum().sum() == 0):\n    print('There is no missing value in training set')\nelse:\n    print('There are ' + str(train_copy.isnull().sum().sum()) + ' missing values in training set')","execution_count":7,"outputs":[{"output_type":"stream","text":"There is no missing value in training set\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (test_copy.isnull().sum().sum() == 0):\n    print('There is no missing value in testing set')\nelse:\n    print('There are ' + str(train_copy.isnull().sum().sum()) + ' missing values in testing set')","execution_count":8,"outputs":[{"output_type":"stream","text":"There is no missing value in testing set\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Great! There is no missing value in both training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.describe()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"              target          var_0      ...              var_198        var_199\ncount  200000.000000  200000.000000      ...        200000.000000  200000.000000\nmean        0.100490      10.679914      ...            15.870720      -3.326537\nstd         0.300653       3.040051      ...             3.010945      10.438015\nmin         0.000000       0.408400      ...             6.299300     -38.852800\n25%         0.000000       8.453850      ...            13.829700     -11.208475\n50%         0.000000      10.524750      ...            15.934050      -2.819550\n75%         0.000000      12.758200      ...            18.064725       4.836800\nmax         1.000000      20.315000      ...            26.079100      28.500700\n\n[8 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.100490</td>\n      <td>10.679914</td>\n      <td>-1.627622</td>\n      <td>10.715192</td>\n      <td>6.796529</td>\n      <td>11.078333</td>\n      <td>-5.065317</td>\n      <td>5.408949</td>\n      <td>16.545850</td>\n      <td>0.284162</td>\n      <td>7.567236</td>\n      <td>0.394340</td>\n      <td>-3.245596</td>\n      <td>14.023978</td>\n      <td>8.530232</td>\n      <td>7.537606</td>\n      <td>14.573126</td>\n      <td>9.333264</td>\n      <td>-5.696731</td>\n      <td>15.244013</td>\n      <td>12.438567</td>\n      <td>13.290894</td>\n      <td>17.257883</td>\n      <td>4.305430</td>\n      <td>3.019540</td>\n      <td>10.584400</td>\n      <td>13.667496</td>\n      <td>-4.055133</td>\n      <td>-1.137908</td>\n      <td>5.532980</td>\n      <td>5.053874</td>\n      <td>-7.687740</td>\n      <td>10.393046</td>\n      <td>-0.512886</td>\n      <td>14.774147</td>\n      <td>11.434250</td>\n      <td>3.842499</td>\n      <td>2.187230</td>\n      <td>5.868899</td>\n      <td>10.642131</td>\n      <td>...</td>\n      <td>24.259300</td>\n      <td>5.633293</td>\n      <td>5.362896</td>\n      <td>11.002170</td>\n      <td>-2.871906</td>\n      <td>19.315753</td>\n      <td>2.963335</td>\n      <td>-4.151155</td>\n      <td>4.937124</td>\n      <td>5.636008</td>\n      <td>-0.004962</td>\n      <td>-0.831777</td>\n      <td>19.817094</td>\n      <td>-0.677967</td>\n      <td>20.210677</td>\n      <td>11.640613</td>\n      <td>-2.799585</td>\n      <td>11.882933</td>\n      <td>-1.014064</td>\n      <td>2.591444</td>\n      <td>-2.741666</td>\n      <td>10.085518</td>\n      <td>0.719109</td>\n      <td>8.769088</td>\n      <td>12.756676</td>\n      <td>-3.983261</td>\n      <td>8.970274</td>\n      <td>-10.335043</td>\n      <td>15.377174</td>\n      <td>0.746072</td>\n      <td>3.234440</td>\n      <td>7.438408</td>\n      <td>1.927839</td>\n      <td>3.331774</td>\n      <td>17.993784</td>\n      <td>-0.142088</td>\n      <td>2.303335</td>\n      <td>8.908158</td>\n      <td>15.870720</td>\n      <td>-3.326537</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.300653</td>\n      <td>3.040051</td>\n      <td>4.050044</td>\n      <td>2.640894</td>\n      <td>2.043319</td>\n      <td>1.623150</td>\n      <td>7.863267</td>\n      <td>0.866607</td>\n      <td>3.418076</td>\n      <td>3.332634</td>\n      <td>1.235070</td>\n      <td>5.500793</td>\n      <td>5.970253</td>\n      <td>0.190059</td>\n      <td>4.639536</td>\n      <td>2.247908</td>\n      <td>0.411711</td>\n      <td>2.557421</td>\n      <td>6.712612</td>\n      <td>7.851370</td>\n      <td>7.996694</td>\n      <td>5.876254</td>\n      <td>8.196564</td>\n      <td>2.847958</td>\n      <td>0.526893</td>\n      <td>3.777245</td>\n      <td>0.285535</td>\n      <td>5.922210</td>\n      <td>1.523714</td>\n      <td>0.783367</td>\n      <td>2.615942</td>\n      <td>7.965198</td>\n      <td>2.159891</td>\n      <td>2.587830</td>\n      <td>4.322325</td>\n      <td>0.541614</td>\n      <td>5.179559</td>\n      <td>3.119978</td>\n      <td>2.249730</td>\n      <td>4.278903</td>\n      <td>...</td>\n      <td>10.880263</td>\n      <td>0.217938</td>\n      <td>1.419612</td>\n      <td>5.262056</td>\n      <td>5.457784</td>\n      <td>5.024182</td>\n      <td>0.369684</td>\n      <td>7.798020</td>\n      <td>3.105986</td>\n      <td>0.369437</td>\n      <td>4.424621</td>\n      <td>5.378008</td>\n      <td>8.674171</td>\n      <td>5.966674</td>\n      <td>7.136427</td>\n      <td>2.892167</td>\n      <td>7.513939</td>\n      <td>2.628895</td>\n      <td>8.579810</td>\n      <td>2.798956</td>\n      <td>5.261243</td>\n      <td>1.371862</td>\n      <td>8.963434</td>\n      <td>4.474924</td>\n      <td>9.318280</td>\n      <td>4.725167</td>\n      <td>3.189759</td>\n      <td>11.574708</td>\n      <td>3.944604</td>\n      <td>0.976348</td>\n      <td>4.559922</td>\n      <td>3.023272</td>\n      <td>1.478423</td>\n      <td>3.992030</td>\n      <td>3.135162</td>\n      <td>1.429372</td>\n      <td>5.454369</td>\n      <td>0.921625</td>\n      <td>3.010945</td>\n      <td>10.438015</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.408400</td>\n      <td>-15.043400</td>\n      <td>2.117100</td>\n      <td>-0.040200</td>\n      <td>5.074800</td>\n      <td>-32.562600</td>\n      <td>2.347300</td>\n      <td>5.349700</td>\n      <td>-10.505500</td>\n      <td>3.970500</td>\n      <td>-20.731300</td>\n      <td>-26.095000</td>\n      <td>13.434600</td>\n      <td>-6.011100</td>\n      <td>1.013300</td>\n      <td>13.076900</td>\n      <td>0.635100</td>\n      <td>-33.380200</td>\n      <td>-10.664200</td>\n      <td>-12.402500</td>\n      <td>-5.432200</td>\n      <td>-10.089000</td>\n      <td>-5.322500</td>\n      <td>1.209800</td>\n      <td>-0.678400</td>\n      <td>12.720000</td>\n      <td>-24.243100</td>\n      <td>-6.166800</td>\n      <td>2.089600</td>\n      <td>-4.787200</td>\n      <td>-34.798400</td>\n      <td>2.140600</td>\n      <td>-8.986100</td>\n      <td>1.508500</td>\n      <td>9.816900</td>\n      <td>-16.513600</td>\n      <td>-8.095100</td>\n      <td>-1.183400</td>\n      <td>-6.337100</td>\n      <td>...</td>\n      <td>-7.452200</td>\n      <td>4.852600</td>\n      <td>0.623100</td>\n      <td>-6.531700</td>\n      <td>-19.997700</td>\n      <td>3.816700</td>\n      <td>1.851200</td>\n      <td>-35.969500</td>\n      <td>-5.250200</td>\n      <td>4.258800</td>\n      <td>-14.506000</td>\n      <td>-22.479300</td>\n      <td>-11.453300</td>\n      <td>-22.748700</td>\n      <td>-2.995300</td>\n      <td>3.241500</td>\n      <td>-29.116500</td>\n      <td>4.952100</td>\n      <td>-29.273400</td>\n      <td>-7.856100</td>\n      <td>-22.037400</td>\n      <td>5.416500</td>\n      <td>-26.001100</td>\n      <td>-4.808200</td>\n      <td>-18.489700</td>\n      <td>-22.583300</td>\n      <td>-3.022300</td>\n      <td>-47.753600</td>\n      <td>4.412300</td>\n      <td>-2.554300</td>\n      <td>-14.093300</td>\n      <td>-2.691700</td>\n      <td>-3.814500</td>\n      <td>-11.783400</td>\n      <td>8.694400</td>\n      <td>-5.261000</td>\n      <td>-14.209600</td>\n      <td>5.960600</td>\n      <td>6.299300</td>\n      <td>-38.852800</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>8.453850</td>\n      <td>-4.740025</td>\n      <td>8.722475</td>\n      <td>5.254075</td>\n      <td>9.883175</td>\n      <td>-11.200350</td>\n      <td>4.767700</td>\n      <td>13.943800</td>\n      <td>-2.317800</td>\n      <td>6.618800</td>\n      <td>-3.594950</td>\n      <td>-7.510600</td>\n      <td>13.894000</td>\n      <td>5.072800</td>\n      <td>5.781875</td>\n      <td>14.262800</td>\n      <td>7.452275</td>\n      <td>-10.476225</td>\n      <td>9.177950</td>\n      <td>6.276475</td>\n      <td>8.627800</td>\n      <td>11.551000</td>\n      <td>2.182400</td>\n      <td>2.634100</td>\n      <td>7.613000</td>\n      <td>13.456400</td>\n      <td>-8.321725</td>\n      <td>-2.307900</td>\n      <td>4.992100</td>\n      <td>3.171700</td>\n      <td>-13.766175</td>\n      <td>8.870000</td>\n      <td>-2.500875</td>\n      <td>11.456300</td>\n      <td>11.032300</td>\n      <td>0.116975</td>\n      <td>-0.007125</td>\n      <td>4.125475</td>\n      <td>7.591050</td>\n      <td>...</td>\n      <td>15.696125</td>\n      <td>5.470500</td>\n      <td>4.326100</td>\n      <td>7.029600</td>\n      <td>-7.094025</td>\n      <td>15.744550</td>\n      <td>2.699000</td>\n      <td>-9.643100</td>\n      <td>2.703200</td>\n      <td>5.374600</td>\n      <td>-3.258500</td>\n      <td>-4.720350</td>\n      <td>13.731775</td>\n      <td>-5.009525</td>\n      <td>15.064600</td>\n      <td>9.371600</td>\n      <td>-8.386500</td>\n      <td>9.808675</td>\n      <td>-7.395700</td>\n      <td>0.625575</td>\n      <td>-6.673900</td>\n      <td>9.084700</td>\n      <td>-6.064425</td>\n      <td>5.423100</td>\n      <td>5.663300</td>\n      <td>-7.360000</td>\n      <td>6.715200</td>\n      <td>-19.205125</td>\n      <td>12.501550</td>\n      <td>0.014900</td>\n      <td>-0.058825</td>\n      <td>5.157400</td>\n      <td>0.889775</td>\n      <td>0.584600</td>\n      <td>15.629800</td>\n      <td>-1.170700</td>\n      <td>-1.946925</td>\n      <td>8.252800</td>\n      <td>13.829700</td>\n      <td>-11.208475</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>10.524750</td>\n      <td>-1.608050</td>\n      <td>10.580000</td>\n      <td>6.825000</td>\n      <td>11.108250</td>\n      <td>-4.833150</td>\n      <td>5.385100</td>\n      <td>16.456800</td>\n      <td>0.393700</td>\n      <td>7.629600</td>\n      <td>0.487300</td>\n      <td>-3.286950</td>\n      <td>14.025500</td>\n      <td>8.604250</td>\n      <td>7.520300</td>\n      <td>14.574100</td>\n      <td>9.232050</td>\n      <td>-5.666350</td>\n      <td>15.196250</td>\n      <td>12.453900</td>\n      <td>13.196800</td>\n      <td>17.234250</td>\n      <td>4.275150</td>\n      <td>3.008650</td>\n      <td>10.380350</td>\n      <td>13.662500</td>\n      <td>-4.196900</td>\n      <td>-1.132100</td>\n      <td>5.534850</td>\n      <td>4.950200</td>\n      <td>-7.411750</td>\n      <td>10.365650</td>\n      <td>-0.497650</td>\n      <td>14.576000</td>\n      <td>11.435200</td>\n      <td>3.917750</td>\n      <td>2.198000</td>\n      <td>5.900650</td>\n      <td>10.562700</td>\n      <td>...</td>\n      <td>23.864500</td>\n      <td>5.633500</td>\n      <td>5.359700</td>\n      <td>10.788700</td>\n      <td>-2.637800</td>\n      <td>19.270800</td>\n      <td>2.960200</td>\n      <td>-4.011600</td>\n      <td>4.761600</td>\n      <td>5.634300</td>\n      <td>0.002800</td>\n      <td>-0.807350</td>\n      <td>19.748000</td>\n      <td>-0.569750</td>\n      <td>20.206100</td>\n      <td>11.679800</td>\n      <td>-2.538450</td>\n      <td>11.737250</td>\n      <td>-0.942050</td>\n      <td>2.512300</td>\n      <td>-2.688800</td>\n      <td>10.036050</td>\n      <td>0.720200</td>\n      <td>8.600000</td>\n      <td>12.521000</td>\n      <td>-3.946950</td>\n      <td>8.902150</td>\n      <td>-10.209750</td>\n      <td>15.239450</td>\n      <td>0.742600</td>\n      <td>3.203600</td>\n      <td>7.347750</td>\n      <td>1.901300</td>\n      <td>3.396350</td>\n      <td>17.957950</td>\n      <td>-0.172700</td>\n      <td>2.408900</td>\n      <td>8.888200</td>\n      <td>15.934050</td>\n      <td>-2.819550</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>12.758200</td>\n      <td>1.358625</td>\n      <td>12.516700</td>\n      <td>8.324100</td>\n      <td>12.261125</td>\n      <td>0.924800</td>\n      <td>6.003000</td>\n      <td>19.102900</td>\n      <td>2.937900</td>\n      <td>8.584425</td>\n      <td>4.382925</td>\n      <td>0.852825</td>\n      <td>14.164200</td>\n      <td>12.274775</td>\n      <td>9.270425</td>\n      <td>14.874500</td>\n      <td>11.055900</td>\n      <td>-0.810775</td>\n      <td>21.013325</td>\n      <td>18.433300</td>\n      <td>17.879400</td>\n      <td>23.089050</td>\n      <td>6.293200</td>\n      <td>3.403800</td>\n      <td>13.479600</td>\n      <td>13.863700</td>\n      <td>-0.090200</td>\n      <td>0.015625</td>\n      <td>6.093700</td>\n      <td>6.798925</td>\n      <td>-1.443450</td>\n      <td>11.885000</td>\n      <td>1.469100</td>\n      <td>18.097125</td>\n      <td>11.844400</td>\n      <td>7.487725</td>\n      <td>4.460400</td>\n      <td>7.542400</td>\n      <td>13.598925</td>\n      <td>...</td>\n      <td>32.622850</td>\n      <td>5.792000</td>\n      <td>6.371200</td>\n      <td>14.623900</td>\n      <td>1.323600</td>\n      <td>23.024025</td>\n      <td>3.241500</td>\n      <td>1.318725</td>\n      <td>7.020025</td>\n      <td>5.905400</td>\n      <td>3.096400</td>\n      <td>2.956800</td>\n      <td>25.907725</td>\n      <td>3.619900</td>\n      <td>25.641225</td>\n      <td>13.745500</td>\n      <td>2.704400</td>\n      <td>13.931300</td>\n      <td>5.338750</td>\n      <td>4.391125</td>\n      <td>0.996200</td>\n      <td>11.011300</td>\n      <td>7.499175</td>\n      <td>12.127425</td>\n      <td>19.456150</td>\n      <td>-0.590650</td>\n      <td>11.193800</td>\n      <td>-1.466000</td>\n      <td>18.345225</td>\n      <td>1.482900</td>\n      <td>6.406200</td>\n      <td>9.512525</td>\n      <td>2.949500</td>\n      <td>6.205800</td>\n      <td>20.396525</td>\n      <td>0.829600</td>\n      <td>6.556725</td>\n      <td>9.593300</td>\n      <td>18.064725</td>\n      <td>4.836800</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>20.315000</td>\n      <td>10.376800</td>\n      <td>19.353000</td>\n      <td>13.188300</td>\n      <td>16.671400</td>\n      <td>17.251600</td>\n      <td>8.447700</td>\n      <td>27.691800</td>\n      <td>10.151300</td>\n      <td>11.150600</td>\n      <td>18.670200</td>\n      <td>17.188700</td>\n      <td>14.654500</td>\n      <td>22.331500</td>\n      <td>14.937700</td>\n      <td>15.863300</td>\n      <td>17.950600</td>\n      <td>19.025900</td>\n      <td>41.748000</td>\n      <td>35.183000</td>\n      <td>31.285900</td>\n      <td>49.044300</td>\n      <td>14.594500</td>\n      <td>4.875200</td>\n      <td>25.446000</td>\n      <td>14.654600</td>\n      <td>15.675100</td>\n      <td>3.243100</td>\n      <td>8.787400</td>\n      <td>13.143100</td>\n      <td>15.651500</td>\n      <td>20.171900</td>\n      <td>6.787100</td>\n      <td>29.546600</td>\n      <td>13.287800</td>\n      <td>21.528900</td>\n      <td>14.245600</td>\n      <td>11.863800</td>\n      <td>29.823500</td>\n      <td>...</td>\n      <td>58.394200</td>\n      <td>6.309900</td>\n      <td>10.134400</td>\n      <td>27.564800</td>\n      <td>12.119300</td>\n      <td>38.332200</td>\n      <td>4.220400</td>\n      <td>21.276600</td>\n      <td>14.886100</td>\n      <td>7.089000</td>\n      <td>16.731900</td>\n      <td>17.917300</td>\n      <td>53.591900</td>\n      <td>18.855400</td>\n      <td>43.546800</td>\n      <td>20.854800</td>\n      <td>20.245200</td>\n      <td>20.596500</td>\n      <td>29.841300</td>\n      <td>13.448700</td>\n      <td>12.750500</td>\n      <td>14.393900</td>\n      <td>29.248700</td>\n      <td>23.704900</td>\n      <td>44.363400</td>\n      <td>12.997500</td>\n      <td>21.739200</td>\n      <td>22.786100</td>\n      <td>29.330300</td>\n      <td>4.034100</td>\n      <td>18.440900</td>\n      <td>16.716500</td>\n      <td>8.402400</td>\n      <td>18.281800</td>\n      <td>27.928800</td>\n      <td>4.272900</td>\n      <td>18.321500</td>\n      <td>12.000400</td>\n      <td>26.079100</td>\n      <td>28.500700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_copy.describe()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"               var_0          var_1      ...              var_198        var_199\ncount  200000.000000  200000.000000      ...        200000.000000  200000.000000\nmean       10.658737      -1.624244      ...            15.869184      -3.246342\nstd         3.036716       4.040509      ...             3.008717      10.398589\nmin         0.188700     -15.043400      ...             6.584000     -39.457800\n25%         8.442975      -4.700125      ...            13.847275     -11.124000\n50%        10.513800      -1.590500      ...            15.943400      -2.725950\n75%        12.739600       1.343400      ...            18.045200       4.935400\nmax        22.323400       9.385100      ...            26.538400      27.907400\n\n[8 rows x 200 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>var_39</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.00000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.658737</td>\n      <td>-1.624244</td>\n      <td>10.707452</td>\n      <td>6.788214</td>\n      <td>11.076399</td>\n      <td>-5.050558</td>\n      <td>5.415164</td>\n      <td>16.529143</td>\n      <td>0.277135</td>\n      <td>7.569407</td>\n      <td>0.371335</td>\n      <td>-3.268551</td>\n      <td>14.022662</td>\n      <td>8.540872</td>\n      <td>7.532703</td>\n      <td>14.573704</td>\n      <td>9.321669</td>\n      <td>-5.70445</td>\n      <td>15.265776</td>\n      <td>12.456675</td>\n      <td>13.298428</td>\n      <td>17.230598</td>\n      <td>4.299010</td>\n      <td>3.019707</td>\n      <td>10.567479</td>\n      <td>13.666970</td>\n      <td>-3.983721</td>\n      <td>-1.129536</td>\n      <td>5.530656</td>\n      <td>5.047247</td>\n      <td>-7.687695</td>\n      <td>10.404920</td>\n      <td>-0.524830</td>\n      <td>14.762686</td>\n      <td>11.434861</td>\n      <td>3.870130</td>\n      <td>2.213288</td>\n      <td>5.875048</td>\n      <td>10.647806</td>\n      <td>0.672667</td>\n      <td>...</td>\n      <td>24.146181</td>\n      <td>5.635300</td>\n      <td>5.360975</td>\n      <td>11.026376</td>\n      <td>-2.857328</td>\n      <td>19.320760</td>\n      <td>2.962821</td>\n      <td>-4.189133</td>\n      <td>4.930356</td>\n      <td>5.633716</td>\n      <td>-0.020824</td>\n      <td>-0.805148</td>\n      <td>19.779528</td>\n      <td>-0.666240</td>\n      <td>20.264135</td>\n      <td>11.635715</td>\n      <td>-2.776134</td>\n      <td>11.864538</td>\n      <td>-0.949318</td>\n      <td>2.582604</td>\n      <td>-2.722636</td>\n      <td>10.080827</td>\n      <td>0.651432</td>\n      <td>8.768929</td>\n      <td>12.719302</td>\n      <td>-3.963045</td>\n      <td>8.978800</td>\n      <td>-10.291919</td>\n      <td>15.366094</td>\n      <td>0.755673</td>\n      <td>3.189766</td>\n      <td>7.458269</td>\n      <td>1.925944</td>\n      <td>3.322016</td>\n      <td>17.996967</td>\n      <td>-0.133657</td>\n      <td>2.290899</td>\n      <td>8.912428</td>\n      <td>15.869184</td>\n      <td>-3.246342</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.036716</td>\n      <td>4.040509</td>\n      <td>2.633888</td>\n      <td>2.052724</td>\n      <td>1.616456</td>\n      <td>7.869293</td>\n      <td>0.864686</td>\n      <td>3.424482</td>\n      <td>3.333375</td>\n      <td>1.231865</td>\n      <td>5.508661</td>\n      <td>5.961443</td>\n      <td>0.190071</td>\n      <td>4.628712</td>\n      <td>2.255257</td>\n      <td>0.411592</td>\n      <td>2.544860</td>\n      <td>6.74646</td>\n      <td>7.846983</td>\n      <td>7.989812</td>\n      <td>5.884245</td>\n      <td>8.199877</td>\n      <td>2.844023</td>\n      <td>0.527951</td>\n      <td>3.771047</td>\n      <td>0.285454</td>\n      <td>5.945853</td>\n      <td>1.524765</td>\n      <td>0.785618</td>\n      <td>2.610078</td>\n      <td>7.971581</td>\n      <td>2.156324</td>\n      <td>2.588700</td>\n      <td>4.325727</td>\n      <td>0.541040</td>\n      <td>5.170614</td>\n      <td>3.120685</td>\n      <td>2.257235</td>\n      <td>4.260820</td>\n      <td>4.078592</td>\n      <td>...</td>\n      <td>10.876184</td>\n      <td>0.217936</td>\n      <td>1.426064</td>\n      <td>5.268894</td>\n      <td>5.457937</td>\n      <td>5.039303</td>\n      <td>0.370668</td>\n      <td>7.827428</td>\n      <td>3.086443</td>\n      <td>0.365750</td>\n      <td>4.417876</td>\n      <td>5.378492</td>\n      <td>8.678024</td>\n      <td>5.987419</td>\n      <td>7.141816</td>\n      <td>2.884821</td>\n      <td>7.557001</td>\n      <td>2.626556</td>\n      <td>8.570314</td>\n      <td>2.803890</td>\n      <td>5.225554</td>\n      <td>1.369546</td>\n      <td>8.961936</td>\n      <td>4.464461</td>\n      <td>9.316889</td>\n      <td>4.724641</td>\n      <td>3.206635</td>\n      <td>11.562352</td>\n      <td>3.929227</td>\n      <td>0.976123</td>\n      <td>4.551239</td>\n      <td>3.025189</td>\n      <td>1.479966</td>\n      <td>3.995599</td>\n      <td>3.140652</td>\n      <td>1.429678</td>\n      <td>5.446346</td>\n      <td>0.920904</td>\n      <td>3.008717</td>\n      <td>10.398589</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.188700</td>\n      <td>-15.043400</td>\n      <td>2.355200</td>\n      <td>-0.022400</td>\n      <td>5.484400</td>\n      <td>-27.767000</td>\n      <td>2.216400</td>\n      <td>5.713700</td>\n      <td>-9.956000</td>\n      <td>4.243300</td>\n      <td>-22.672400</td>\n      <td>-25.811800</td>\n      <td>13.424500</td>\n      <td>-4.741300</td>\n      <td>0.670300</td>\n      <td>13.203400</td>\n      <td>0.314300</td>\n      <td>-28.90690</td>\n      <td>-11.324200</td>\n      <td>-12.699400</td>\n      <td>-2.634600</td>\n      <td>-9.940600</td>\n      <td>-5.164000</td>\n      <td>1.390600</td>\n      <td>-0.731300</td>\n      <td>12.749600</td>\n      <td>-24.536100</td>\n      <td>-6.040900</td>\n      <td>2.842500</td>\n      <td>-4.421500</td>\n      <td>-34.054800</td>\n      <td>1.309200</td>\n      <td>-8.209000</td>\n      <td>1.691100</td>\n      <td>9.776400</td>\n      <td>-16.923800</td>\n      <td>-10.466800</td>\n      <td>-0.885100</td>\n      <td>-5.368300</td>\n      <td>-14.083700</td>\n      <td>...</td>\n      <td>-8.925700</td>\n      <td>4.910600</td>\n      <td>0.106200</td>\n      <td>-6.093700</td>\n      <td>-21.514000</td>\n      <td>3.667300</td>\n      <td>1.813100</td>\n      <td>-37.176400</td>\n      <td>-5.405700</td>\n      <td>4.291500</td>\n      <td>-15.593200</td>\n      <td>-20.393600</td>\n      <td>-11.796600</td>\n      <td>-21.342800</td>\n      <td>-2.485400</td>\n      <td>2.951200</td>\n      <td>-29.838400</td>\n      <td>5.025300</td>\n      <td>-29.118500</td>\n      <td>-7.767400</td>\n      <td>-20.610600</td>\n      <td>5.346000</td>\n      <td>-28.092800</td>\n      <td>-5.476800</td>\n      <td>-17.011400</td>\n      <td>-22.467000</td>\n      <td>-2.303800</td>\n      <td>-47.306400</td>\n      <td>4.429100</td>\n      <td>-2.511500</td>\n      <td>-14.093300</td>\n      <td>-2.407000</td>\n      <td>-3.340900</td>\n      <td>-11.413100</td>\n      <td>9.382800</td>\n      <td>-4.911900</td>\n      <td>-13.944200</td>\n      <td>6.169600</td>\n      <td>6.584000</td>\n      <td>-39.457800</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.442975</td>\n      <td>-4.700125</td>\n      <td>8.735600</td>\n      <td>5.230500</td>\n      <td>9.891075</td>\n      <td>-11.201400</td>\n      <td>4.772600</td>\n      <td>13.933900</td>\n      <td>-2.303900</td>\n      <td>6.623800</td>\n      <td>-3.626000</td>\n      <td>-7.522000</td>\n      <td>13.891000</td>\n      <td>5.073375</td>\n      <td>5.769500</td>\n      <td>14.262400</td>\n      <td>7.454400</td>\n      <td>-10.49790</td>\n      <td>9.237700</td>\n      <td>6.322300</td>\n      <td>8.589600</td>\n      <td>11.511500</td>\n      <td>2.178300</td>\n      <td>2.633300</td>\n      <td>7.610750</td>\n      <td>13.456200</td>\n      <td>-8.265500</td>\n      <td>-2.299000</td>\n      <td>4.986275</td>\n      <td>3.166200</td>\n      <td>-13.781900</td>\n      <td>8.880600</td>\n      <td>-2.518200</td>\n      <td>11.440500</td>\n      <td>11.033200</td>\n      <td>0.162400</td>\n      <td>0.016900</td>\n      <td>4.120700</td>\n      <td>7.601375</td>\n      <td>-2.170300</td>\n      <td>...</td>\n      <td>15.567800</td>\n      <td>5.473000</td>\n      <td>4.308175</td>\n      <td>7.067775</td>\n      <td>-7.051200</td>\n      <td>15.751000</td>\n      <td>2.696500</td>\n      <td>-9.712000</td>\n      <td>2.729800</td>\n      <td>5.375200</td>\n      <td>-3.250000</td>\n      <td>-4.678450</td>\n      <td>13.722200</td>\n      <td>-4.998900</td>\n      <td>15.126500</td>\n      <td>9.382575</td>\n      <td>-8.408100</td>\n      <td>9.793700</td>\n      <td>-7.337925</td>\n      <td>0.605200</td>\n      <td>-6.604425</td>\n      <td>9.081000</td>\n      <td>-6.154625</td>\n      <td>5.432025</td>\n      <td>5.631700</td>\n      <td>-7.334000</td>\n      <td>6.705300</td>\n      <td>-19.136225</td>\n      <td>12.492600</td>\n      <td>0.019400</td>\n      <td>-0.095000</td>\n      <td>5.166500</td>\n      <td>0.882975</td>\n      <td>0.587600</td>\n      <td>15.634775</td>\n      <td>-1.160700</td>\n      <td>-1.948600</td>\n      <td>8.260075</td>\n      <td>13.847275</td>\n      <td>-11.124000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.513800</td>\n      <td>-1.590500</td>\n      <td>10.560700</td>\n      <td>6.822350</td>\n      <td>11.099750</td>\n      <td>-4.834100</td>\n      <td>5.391600</td>\n      <td>16.422700</td>\n      <td>0.372000</td>\n      <td>7.632000</td>\n      <td>0.491850</td>\n      <td>-3.314950</td>\n      <td>14.024600</td>\n      <td>8.617400</td>\n      <td>7.496950</td>\n      <td>14.572700</td>\n      <td>9.228900</td>\n      <td>-5.69820</td>\n      <td>15.203200</td>\n      <td>12.484250</td>\n      <td>13.218650</td>\n      <td>17.211300</td>\n      <td>4.269000</td>\n      <td>3.008000</td>\n      <td>10.344300</td>\n      <td>13.661200</td>\n      <td>-4.125800</td>\n      <td>-1.127800</td>\n      <td>5.529900</td>\n      <td>4.953100</td>\n      <td>-7.409000</td>\n      <td>10.385350</td>\n      <td>-0.535200</td>\n      <td>14.561400</td>\n      <td>11.435100</td>\n      <td>3.947500</td>\n      <td>2.219250</td>\n      <td>5.909800</td>\n      <td>10.563750</td>\n      <td>0.700850</td>\n      <td>...</td>\n      <td>23.734400</td>\n      <td>5.636600</td>\n      <td>5.359800</td>\n      <td>10.820600</td>\n      <td>-2.618500</td>\n      <td>19.290300</td>\n      <td>2.961100</td>\n      <td>-4.080550</td>\n      <td>4.749300</td>\n      <td>5.632800</td>\n      <td>0.008000</td>\n      <td>-0.782800</td>\n      <td>19.723750</td>\n      <td>-0.564750</td>\n      <td>20.287200</td>\n      <td>11.668400</td>\n      <td>-2.515400</td>\n      <td>11.707650</td>\n      <td>-0.868300</td>\n      <td>2.496500</td>\n      <td>-2.671600</td>\n      <td>10.027200</td>\n      <td>0.675100</td>\n      <td>8.602300</td>\n      <td>12.493350</td>\n      <td>-3.927300</td>\n      <td>8.912850</td>\n      <td>-10.166800</td>\n      <td>15.211000</td>\n      <td>0.759700</td>\n      <td>3.162400</td>\n      <td>7.379000</td>\n      <td>1.892600</td>\n      <td>3.428500</td>\n      <td>17.977600</td>\n      <td>-0.162000</td>\n      <td>2.403600</td>\n      <td>8.892800</td>\n      <td>15.943400</td>\n      <td>-2.725950</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.739600</td>\n      <td>1.343400</td>\n      <td>12.495025</td>\n      <td>8.327600</td>\n      <td>12.253400</td>\n      <td>0.942575</td>\n      <td>6.005800</td>\n      <td>19.094550</td>\n      <td>2.930025</td>\n      <td>8.584825</td>\n      <td>4.362400</td>\n      <td>0.832525</td>\n      <td>14.162900</td>\n      <td>12.270900</td>\n      <td>9.271125</td>\n      <td>14.875600</td>\n      <td>11.035500</td>\n      <td>-0.81160</td>\n      <td>21.014500</td>\n      <td>18.441950</td>\n      <td>17.914200</td>\n      <td>23.031600</td>\n      <td>6.278200</td>\n      <td>3.405700</td>\n      <td>13.467500</td>\n      <td>13.862800</td>\n      <td>-0.000700</td>\n      <td>0.026200</td>\n      <td>6.092200</td>\n      <td>6.793425</td>\n      <td>-1.464000</td>\n      <td>11.890900</td>\n      <td>1.460225</td>\n      <td>18.084425</td>\n      <td>11.843100</td>\n      <td>7.513375</td>\n      <td>4.485600</td>\n      <td>7.556750</td>\n      <td>13.615200</td>\n      <td>3.654800</td>\n      <td>...</td>\n      <td>32.495275</td>\n      <td>5.794100</td>\n      <td>6.367900</td>\n      <td>14.645800</td>\n      <td>1.330300</td>\n      <td>23.040250</td>\n      <td>3.241600</td>\n      <td>1.313125</td>\n      <td>7.004400</td>\n      <td>5.898900</td>\n      <td>3.070100</td>\n      <td>2.982900</td>\n      <td>25.849600</td>\n      <td>3.652625</td>\n      <td>25.720000</td>\n      <td>13.748500</td>\n      <td>2.737700</td>\n      <td>13.902500</td>\n      <td>5.423900</td>\n      <td>4.384725</td>\n      <td>1.024600</td>\n      <td>11.002000</td>\n      <td>7.474700</td>\n      <td>12.126700</td>\n      <td>19.437600</td>\n      <td>-0.626300</td>\n      <td>11.227100</td>\n      <td>-1.438800</td>\n      <td>18.322925</td>\n      <td>1.495400</td>\n      <td>6.336475</td>\n      <td>9.531100</td>\n      <td>2.956000</td>\n      <td>6.174200</td>\n      <td>20.391725</td>\n      <td>0.837900</td>\n      <td>6.519800</td>\n      <td>9.595900</td>\n      <td>18.045200</td>\n      <td>4.935400</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>22.323400</td>\n      <td>9.385100</td>\n      <td>18.714100</td>\n      <td>13.142000</td>\n      <td>16.037100</td>\n      <td>17.253700</td>\n      <td>8.302500</td>\n      <td>28.292800</td>\n      <td>9.665500</td>\n      <td>11.003600</td>\n      <td>20.214500</td>\n      <td>16.771300</td>\n      <td>14.682000</td>\n      <td>21.605100</td>\n      <td>14.723100</td>\n      <td>15.798000</td>\n      <td>17.368700</td>\n      <td>19.15090</td>\n      <td>38.929000</td>\n      <td>35.432300</td>\n      <td>32.075800</td>\n      <td>47.417900</td>\n      <td>14.042600</td>\n      <td>5.024600</td>\n      <td>23.839600</td>\n      <td>14.596400</td>\n      <td>13.456400</td>\n      <td>3.371300</td>\n      <td>8.459900</td>\n      <td>12.953200</td>\n      <td>14.391500</td>\n      <td>19.471900</td>\n      <td>6.949600</td>\n      <td>29.247500</td>\n      <td>13.225100</td>\n      <td>22.318300</td>\n      <td>13.094100</td>\n      <td>12.014900</td>\n      <td>27.142700</td>\n      <td>14.167300</td>\n      <td>...</td>\n      <td>64.291100</td>\n      <td>6.343700</td>\n      <td>10.194200</td>\n      <td>27.150300</td>\n      <td>11.885500</td>\n      <td>37.026700</td>\n      <td>4.216200</td>\n      <td>20.524400</td>\n      <td>14.983600</td>\n      <td>6.936400</td>\n      <td>16.846500</td>\n      <td>17.269200</td>\n      <td>53.426500</td>\n      <td>19.237600</td>\n      <td>42.758200</td>\n      <td>19.892200</td>\n      <td>19.677300</td>\n      <td>20.007800</td>\n      <td>27.956800</td>\n      <td>14.067500</td>\n      <td>13.991000</td>\n      <td>14.055900</td>\n      <td>28.255300</td>\n      <td>25.568500</td>\n      <td>44.363400</td>\n      <td>12.488600</td>\n      <td>21.699900</td>\n      <td>23.569900</td>\n      <td>28.885200</td>\n      <td>3.780300</td>\n      <td>20.359000</td>\n      <td>16.716500</td>\n      <td>8.005000</td>\n      <td>17.632600</td>\n      <td>27.947800</td>\n      <td>4.545400</td>\n      <td>15.920700</td>\n      <td>12.275800</td>\n      <td>26.538400</td>\n      <td>27.907400</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As shown, training data and testing data are quite similar in each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_copy['target'])","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f53a9f7fdd8>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"So there exsists imbalanced data issue with only 10% records responding 1."},{"metadata":{},"cell_type":"markdown","source":"To handle this problem, one can just cut '0' responding data to around numbers of positive records using 'sample()' command, but he will loss lots of information, or apply resampling mathod to create positive records.\nAs a result, resampling does not improve the auc much while the former way does improve but not good enough.\nI am going to apply the first method."},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(0)\ntrain_one = train_copy[train_copy['target'] == 1]\n# rows with response 0\ntrain_zero = train_copy[train_copy['target'] == 0]\n# rows equal to rows of 1\ntrain_zero_equal = train_zero.sample(n=40000)\ntrain = pd.concat([train_one, train_zero_equal], axis=0)\ntrain_y = train['target'].values\ntrain_X_column_name = train.drop(['target', 'ID_code'], axis=1).columns\ntrain_X = train.drop(['target', 'ID_code'], axis=1).values\ntest_X = test_copy.drop(['ID_code'], axis=1).values\ntrain_X_copy = train_X.copy()\ntest_X_copy = test_X.copy()","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Until now, I have separated the training data into features and targets, and picked the features from testing data. \nAnd I have taken random 40000 samples from 0 responding data, after parameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_X = scaler.fit_transform(train_X)\ntest_X = scaler.transform(test_X)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just scaled the training data among features."},{"metadata":{},"cell_type":"markdown","source":"Let's try very basic neural network model first."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(32, input_dim=200, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":14,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"And do K fold validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold, (train_idx, valid_idx) in enumerate(folds.split(train_X, train_y)):\n    train_xx, valid_xx = train_X[train_idx], train_X[valid_idx]\n    train_t, valid_t = train_y[train_idx], train_y[valid_idx]\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(train_xx, train_t, validation_data=(valid_xx, valid_t), epochs=10, batch_size=500)","execution_count":15,"outputs":[{"output_type":"stream","text":"Train on 48078 samples, validate on 12020 samples\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/10\n48078/48078 [==============================] - 1s 17us/sample - loss: 0.5699 - acc: 0.6971 - val_loss: 0.4643 - val_acc: 0.7822\nEpoch 2/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4518 - acc: 0.7900 - val_loss: 0.4507 - val_acc: 0.7923\nEpoch 3/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4408 - acc: 0.7970 - val_loss: 0.4479 - val_acc: 0.7953\nEpoch 4/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4343 - acc: 0.8009 - val_loss: 0.4466 - val_acc: 0.7953\nEpoch 5/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4286 - acc: 0.8044 - val_loss: 0.4459 - val_acc: 0.7959\nEpoch 6/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.4238 - acc: 0.8074 - val_loss: 0.4443 - val_acc: 0.7968\nEpoch 7/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4194 - acc: 0.8094 - val_loss: 0.4439 - val_acc: 0.7963\nEpoch 8/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4155 - acc: 0.8117 - val_loss: 0.4438 - val_acc: 0.7963\nEpoch 9/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4116 - acc: 0.8136 - val_loss: 0.4441 - val_acc: 0.7983\nEpoch 10/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.4078 - acc: 0.8161 - val_loss: 0.4441 - val_acc: 0.7988\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 1s 12us/sample - loss: 0.4147 - acc: 0.8129 - val_loss: 0.4066 - val_acc: 0.8166\nEpoch 2/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4089 - acc: 0.8162 - val_loss: 0.4100 - val_acc: 0.8156\nEpoch 3/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4049 - acc: 0.8181 - val_loss: 0.4112 - val_acc: 0.8136\nEpoch 4/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.4009 - acc: 0.8210 - val_loss: 0.4127 - val_acc: 0.8121\nEpoch 5/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3977 - acc: 0.8223 - val_loss: 0.4148 - val_acc: 0.8116\nEpoch 6/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3944 - acc: 0.8248 - val_loss: 0.4171 - val_acc: 0.8112\nEpoch 7/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3913 - acc: 0.8261 - val_loss: 0.4195 - val_acc: 0.8098\nEpoch 8/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3881 - acc: 0.8275 - val_loss: 0.4214 - val_acc: 0.8076\nEpoch 9/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3849 - acc: 0.8286 - val_loss: 0.4232 - val_acc: 0.8066\nEpoch 10/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3823 - acc: 0.8307 - val_loss: 0.4258 - val_acc: 0.8051\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 1s 13us/sample - loss: 0.3923 - acc: 0.8256 - val_loss: 0.3803 - val_acc: 0.8293\nEpoch 2/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3868 - acc: 0.8284 - val_loss: 0.3842 - val_acc: 0.8284\nEpoch 3/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3835 - acc: 0.8299 - val_loss: 0.3898 - val_acc: 0.8243\nEpoch 4/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3804 - acc: 0.8320 - val_loss: 0.3933 - val_acc: 0.8235\nEpoch 5/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3778 - acc: 0.8331 - val_loss: 0.3969 - val_acc: 0.8220\nEpoch 6/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3752 - acc: 0.8356 - val_loss: 0.3998 - val_acc: 0.8187\nEpoch 7/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3732 - acc: 0.8352 - val_loss: 0.4038 - val_acc: 0.8188\nEpoch 8/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3706 - acc: 0.8374 - val_loss: 0.4072 - val_acc: 0.8166\nEpoch 9/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3685 - acc: 0.8388 - val_loss: 0.4113 - val_acc: 0.8151\nEpoch 10/10\n48078/48078 [==============================] - 0s 7us/sample - loss: 0.3668 - acc: 0.8389 - val_loss: 0.4143 - val_acc: 0.8135\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 1s 13us/sample - loss: 0.3818 - acc: 0.8319 - val_loss: 0.3547 - val_acc: 0.8434\nEpoch 2/10\n48079/48079 [==============================] - 0s 7us/sample - loss: 0.3766 - acc: 0.8337 - val_loss: 0.3603 - val_acc: 0.8416\nEpoch 3/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3736 - acc: 0.8369 - val_loss: 0.3657 - val_acc: 0.8381\nEpoch 4/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3715 - acc: 0.8377 - val_loss: 0.3690 - val_acc: 0.8363\nEpoch 5/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3692 - acc: 0.8380 - val_loss: 0.3753 - val_acc: 0.8326\nEpoch 6/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3674 - acc: 0.8400 - val_loss: 0.3787 - val_acc: 0.8317\nEpoch 7/10\n48079/48079 [==============================] - 0s 7us/sample - loss: 0.3657 - acc: 0.8399 - val_loss: 0.3832 - val_acc: 0.8291\nEpoch 8/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3638 - acc: 0.8406 - val_loss: 0.3860 - val_acc: 0.8260\nEpoch 9/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3625 - acc: 0.8418 - val_loss: 0.3899 - val_acc: 0.8240\nEpoch 10/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3607 - acc: 0.8422 - val_loss: 0.3931 - val_acc: 0.8233\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 1s 14us/sample - loss: 0.3735 - acc: 0.8355 - val_loss: 0.3465 - val_acc: 0.8482\nEpoch 2/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3679 - acc: 0.8386 - val_loss: 0.3530 - val_acc: 0.8457\nEpoch 3/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3654 - acc: 0.8399 - val_loss: 0.3603 - val_acc: 0.8388\nEpoch 4/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3631 - acc: 0.8412 - val_loss: 0.3650 - val_acc: 0.8368\nEpoch 5/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3611 - acc: 0.8425 - val_loss: 0.3698 - val_acc: 0.8355\nEpoch 6/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3595 - acc: 0.8435 - val_loss: 0.3740 - val_acc: 0.8332\nEpoch 7/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3583 - acc: 0.8446 - val_loss: 0.3791 - val_acc: 0.8284\nEpoch 8/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3565 - acc: 0.8452 - val_loss: 0.3834 - val_acc: 0.8273\nEpoch 9/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3552 - acc: 0.8460 - val_loss: 0.3858 - val_acc: 0.8248\nEpoch 10/10\n48079/48079 [==============================] - 0s 8us/sample - loss: 0.3541 - acc: 0.8461 - val_loss: 0.3908 - val_acc: 0.8215\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Then I add one more layer and try again"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(32, input_dim=200, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold, (train_idx, valid_idx) in enumerate(folds.split(train_X, train_y)):\n    train_xx, valid_xx = train_X[train_idx], train_X[valid_idx]\n    train_t, valid_t = train_y[train_idx], train_y[valid_idx]\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(train_xx, train_t, validation_data=(valid_xx, valid_t), epochs=10, batch_size=500)","execution_count":16,"outputs":[{"output_type":"stream","text":"Train on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 1s 16us/sample - loss: 0.5782 - acc: 0.6950 - val_loss: 0.4634 - val_acc: 0.7817\nEpoch 2/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4495 - acc: 0.7932 - val_loss: 0.4467 - val_acc: 0.7933\nEpoch 3/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4352 - acc: 0.8010 - val_loss: 0.4443 - val_acc: 0.7953\nEpoch 4/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4272 - acc: 0.8061 - val_loss: 0.4419 - val_acc: 0.7978\nEpoch 5/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4209 - acc: 0.8097 - val_loss: 0.4415 - val_acc: 0.7975\nEpoch 6/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4156 - acc: 0.8128 - val_loss: 0.4411 - val_acc: 0.7970\nEpoch 7/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4109 - acc: 0.8146 - val_loss: 0.4401 - val_acc: 0.7997\nEpoch 8/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4065 - acc: 0.8171 - val_loss: 0.4411 - val_acc: 0.7985\nEpoch 9/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.4020 - acc: 0.8195 - val_loss: 0.4423 - val_acc: 0.7962\nEpoch 10/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3979 - acc: 0.8231 - val_loss: 0.4438 - val_acc: 0.7955\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 1s 17us/sample - loss: 0.4068 - acc: 0.8164 - val_loss: 0.3974 - val_acc: 0.8200\nEpoch 2/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3999 - acc: 0.8223 - val_loss: 0.4020 - val_acc: 0.8189\nEpoch 3/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3948 - acc: 0.8243 - val_loss: 0.4049 - val_acc: 0.8194\nEpoch 4/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3900 - acc: 0.8275 - val_loss: 0.4088 - val_acc: 0.8163\nEpoch 5/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3860 - acc: 0.8294 - val_loss: 0.4123 - val_acc: 0.8136\nEpoch 6/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3813 - acc: 0.8321 - val_loss: 0.4169 - val_acc: 0.8134\nEpoch 7/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3781 - acc: 0.8340 - val_loss: 0.4191 - val_acc: 0.8084\nEpoch 8/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3737 - acc: 0.8362 - val_loss: 0.4231 - val_acc: 0.8089\nEpoch 9/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3696 - acc: 0.8391 - val_loss: 0.4257 - val_acc: 0.8073\nEpoch 10/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3658 - acc: 0.8401 - val_loss: 0.4316 - val_acc: 0.8046\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 1s 17us/sample - loss: 0.3812 - acc: 0.8323 - val_loss: 0.3643 - val_acc: 0.8404\nEpoch 2/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3731 - acc: 0.8370 - val_loss: 0.3702 - val_acc: 0.8341\nEpoch 3/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3681 - acc: 0.8382 - val_loss: 0.3765 - val_acc: 0.8304\nEpoch 4/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3638 - acc: 0.8411 - val_loss: 0.3823 - val_acc: 0.8265\nEpoch 5/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3602 - acc: 0.8426 - val_loss: 0.3871 - val_acc: 0.8240\nEpoch 6/10\n48078/48078 [==============================] - 0s 8us/sample - loss: 0.3570 - acc: 0.8446 - val_loss: 0.3974 - val_acc: 0.8181\nEpoch 7/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3535 - acc: 0.8471 - val_loss: 0.3981 - val_acc: 0.8182\nEpoch 8/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3499 - acc: 0.8495 - val_loss: 0.4038 - val_acc: 0.8165\nEpoch 9/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3465 - acc: 0.8513 - val_loss: 0.4080 - val_acc: 0.8129\nEpoch 10/10\n48078/48078 [==============================] - 0s 9us/sample - loss: 0.3441 - acc: 0.8513 - val_loss: 0.4116 - val_acc: 0.8122\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 1s 17us/sample - loss: 0.3635 - acc: 0.8406 - val_loss: 0.3324 - val_acc: 0.8570\nEpoch 2/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3548 - acc: 0.8445 - val_loss: 0.3410 - val_acc: 0.8494\nEpoch 3/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3501 - acc: 0.8483 - val_loss: 0.3507 - val_acc: 0.8450\nEpoch 4/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3463 - acc: 0.8487 - val_loss: 0.3579 - val_acc: 0.8418\nEpoch 5/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3433 - acc: 0.8511 - val_loss: 0.3646 - val_acc: 0.8362\nEpoch 6/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3396 - acc: 0.8517 - val_loss: 0.3697 - val_acc: 0.8343\nEpoch 7/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3368 - acc: 0.8538 - val_loss: 0.3761 - val_acc: 0.8299\nEpoch 8/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3342 - acc: 0.8551 - val_loss: 0.3838 - val_acc: 0.8266\nEpoch 9/10\n48079/48079 [==============================] - 0s 10us/sample - loss: 0.3322 - acc: 0.8563 - val_loss: 0.3876 - val_acc: 0.8244\nEpoch 10/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3292 - acc: 0.8583 - val_loss: 0.3924 - val_acc: 0.8210\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 1s 19us/sample - loss: 0.3485 - acc: 0.8464 - val_loss: 0.3187 - val_acc: 0.8622\nEpoch 2/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3389 - acc: 0.8519 - val_loss: 0.3287 - val_acc: 0.8583\nEpoch 3/10\n48079/48079 [==============================] - 0s 10us/sample - loss: 0.3348 - acc: 0.8544 - val_loss: 0.3386 - val_acc: 0.8530\nEpoch 4/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3310 - acc: 0.8566 - val_loss: 0.3456 - val_acc: 0.8492\nEpoch 5/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3278 - acc: 0.8577 - val_loss: 0.3535 - val_acc: 0.8434\nEpoch 6/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3259 - acc: 0.8579 - val_loss: 0.3606 - val_acc: 0.8403\nEpoch 7/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3228 - acc: 0.8599 - val_loss: 0.3651 - val_acc: 0.8377\nEpoch 8/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3208 - acc: 0.8610 - val_loss: 0.3721 - val_acc: 0.8348\nEpoch 9/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3186 - acc: 0.8605 - val_loss: 0.3777 - val_acc: 0.8308\nEpoch 10/10\n48079/48079 [==============================] - 0s 9us/sample - loss: 0.3164 - acc: 0.8636 - val_loss: 0.3827 - val_acc: 0.8296\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"At last, I try convolutional neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = np.expand_dims(train_X, axis=2)\ntest_X = np.expand_dims(test_X, axis=2)\nmodel1 = Sequential()\nmodel1.add(Conv1D(64, kernel_size=2, strides=2, padding='valid', activation='relu',\n                  input_shape=(200, 1)))\nmodel1.add(Conv1D(128, kernel_size=2, strides=2, padding='valid', activation='relu'))\nmodel1.add(Flatten())\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dense(1, activation='sigmoid'))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold, (train_idx, valid_idx) in enumerate(folds.split(train_X, train_y)):\n    train_xx, valid_xx = train_X[train_idx], train_X[valid_idx]\n    train_t, valid_t = train_y[train_idx], train_y[valid_idx]\n    model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model1.fit(train_xx, train_t, validation_data=(valid_xx, valid_t), epochs=10, batch_size=500)","execution_count":17,"outputs":[{"output_type":"stream","text":"Train on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 10s 213us/sample - loss: 0.4876 - acc: 0.7611 - val_loss: 0.4195 - val_acc: 0.8075\nEpoch 2/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.4173 - acc: 0.8105 - val_loss: 0.4144 - val_acc: 0.8095\nEpoch 3/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.4060 - acc: 0.8164 - val_loss: 0.3994 - val_acc: 0.8187\nEpoch 4/10\n48078/48078 [==============================] - 10s 204us/sample - loss: 0.3948 - acc: 0.8253 - val_loss: 0.3920 - val_acc: 0.8211\nEpoch 5/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.3797 - acc: 0.8325 - val_loss: 0.4322 - val_acc: 0.8067\nEpoch 6/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.3761 - acc: 0.8355 - val_loss: 0.3843 - val_acc: 0.8285\nEpoch 7/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.3715 - acc: 0.8351 - val_loss: 0.4167 - val_acc: 0.8117\nEpoch 8/10\n48078/48078 [==============================] - 10s 201us/sample - loss: 0.3742 - acc: 0.8338 - val_loss: 0.3906 - val_acc: 0.8241\nEpoch 9/10\n48078/48078 [==============================] - 10s 201us/sample - loss: 0.3630 - acc: 0.8406 - val_loss: 0.3876 - val_acc: 0.8262\nEpoch 10/10\n48078/48078 [==============================] - 10s 201us/sample - loss: 0.3596 - acc: 0.8427 - val_loss: 0.3982 - val_acc: 0.8218\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 10s 216us/sample - loss: 0.3731 - acc: 0.8342 - val_loss: 0.3611 - val_acc: 0.8389\nEpoch 2/10\n48078/48078 [==============================] - 10s 205us/sample - loss: 0.3572 - acc: 0.8424 - val_loss: 0.3618 - val_acc: 0.8414\nEpoch 3/10\n48078/48078 [==============================] - 10s 206us/sample - loss: 0.3527 - acc: 0.8437 - val_loss: 0.3682 - val_acc: 0.8353\nEpoch 4/10\n48078/48078 [==============================] - 10s 205us/sample - loss: 0.3468 - acc: 0.8480 - val_loss: 0.3687 - val_acc: 0.8349\nEpoch 5/10\n48078/48078 [==============================] - 10s 204us/sample - loss: 0.3437 - acc: 0.8486 - val_loss: 0.3719 - val_acc: 0.8339\nEpoch 6/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.3363 - acc: 0.8529 - val_loss: 0.3823 - val_acc: 0.8350\nEpoch 7/10\n48078/48078 [==============================] - 10s 200us/sample - loss: 0.3297 - acc: 0.8557 - val_loss: 0.4006 - val_acc: 0.8227\nEpoch 8/10\n48078/48078 [==============================] - 10s 207us/sample - loss: 0.3171 - acc: 0.8622 - val_loss: 0.3956 - val_acc: 0.8278\nEpoch 9/10\n48078/48078 [==============================] - 10s 211us/sample - loss: 0.3100 - acc: 0.8662 - val_loss: 0.4031 - val_acc: 0.8261\nEpoch 10/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.2934 - acc: 0.8742 - val_loss: 0.4096 - val_acc: 0.8206\nTrain on 48078 samples, validate on 12020 samples\nEpoch 1/10\n48078/48078 [==============================] - 11s 218us/sample - loss: 0.3127 - acc: 0.8666 - val_loss: 0.2950 - val_acc: 0.8728\nEpoch 2/10\n48078/48078 [==============================] - 10s 204us/sample - loss: 0.2955 - acc: 0.8748 - val_loss: 0.2929 - val_acc: 0.8749\nEpoch 3/10\n48078/48078 [==============================] - 10s 205us/sample - loss: 0.2770 - acc: 0.8853 - val_loss: 0.3031 - val_acc: 0.8688\nEpoch 4/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.2608 - acc: 0.8933 - val_loss: 0.3278 - val_acc: 0.8587\nEpoch 5/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.2467 - acc: 0.9004 - val_loss: 0.3254 - val_acc: 0.8580\nEpoch 6/10\n48078/48078 [==============================] - 10s 205us/sample - loss: 0.2307 - acc: 0.9079 - val_loss: 0.3419 - val_acc: 0.8547\nEpoch 7/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.2063 - acc: 0.9204 - val_loss: 0.3765 - val_acc: 0.8452\nEpoch 8/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.1880 - acc: 0.9282 - val_loss: 0.3882 - val_acc: 0.8444\nEpoch 9/10\n48078/48078 [==============================] - 10s 203us/sample - loss: 0.1648 - acc: 0.9393 - val_loss: 0.4358 - val_acc: 0.8311\nEpoch 10/10\n48078/48078 [==============================] - 10s 202us/sample - loss: 0.1459 - acc: 0.9462 - val_loss: 0.4546 - val_acc: 0.8383\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 10s 216us/sample - loss: 0.2140 - acc: 0.9227 - val_loss: 0.1304 - val_acc: 0.9579\nEpoch 2/10\n48079/48079 [==============================] - 10s 200us/sample - loss: 0.1733 - acc: 0.9399 - val_loss: 0.1377 - val_acc: 0.9484\nEpoch 3/10\n48079/48079 [==============================] - 10s 201us/sample - loss: 0.1500 - acc: 0.9488 - val_loss: 0.1627 - val_acc: 0.9339\nEpoch 4/10\n48079/48079 [==============================] - 10s 200us/sample - loss: 0.1276 - acc: 0.9570 - val_loss: 0.1781 - val_acc: 0.9264\nEpoch 5/10\n48079/48079 [==============================] - 9s 197us/sample - loss: 0.1071 - acc: 0.9649 - val_loss: 0.2126 - val_acc: 0.9127\nEpoch 6/10\n48079/48079 [==============================] - 9s 196us/sample - loss: 0.0867 - acc: 0.9740 - val_loss: 0.2214 - val_acc: 0.9207\nEpoch 7/10\n48079/48079 [==============================] - 10s 198us/sample - loss: 0.0746 - acc: 0.9766 - val_loss: 0.2644 - val_acc: 0.8969\nEpoch 8/10\n48079/48079 [==============================] - 10s 198us/sample - loss: 0.0620 - acc: 0.9813 - val_loss: 0.2564 - val_acc: 0.9031\nEpoch 9/10\n48079/48079 [==============================] - 10s 201us/sample - loss: 0.0478 - acc: 0.9871 - val_loss: 0.2762 - val_acc: 0.9052\nEpoch 10/10\n48079/48079 [==============================] - 10s 200us/sample - loss: 0.0314 - acc: 0.9928 - val_loss: 0.3017 - val_acc: 0.9029\nTrain on 48079 samples, validate on 12019 samples\nEpoch 1/10\n48079/48079 [==============================] - 10s 214us/sample - loss: 0.1124 - acc: 0.9642 - val_loss: 0.0367 - val_acc: 0.9938\nEpoch 2/10\n48079/48079 [==============================] - 10s 206us/sample - loss: 0.0591 - acc: 0.9815 - val_loss: 0.0446 - val_acc: 0.9850\nEpoch 3/10\n48079/48079 [==============================] - 10s 201us/sample - loss: 0.0391 - acc: 0.9904 - val_loss: 0.0419 - val_acc: 0.9874\nEpoch 4/10\n48079/48079 [==============================] - 10s 202us/sample - loss: 0.0254 - acc: 0.9948 - val_loss: 0.0463 - val_acc: 0.9828\nEpoch 5/10\n48079/48079 [==============================] - 10s 200us/sample - loss: 0.0191 - acc: 0.9968 - val_loss: 0.0518 - val_acc: 0.9807\nEpoch 6/10\n48079/48079 [==============================] - 10s 202us/sample - loss: 0.0143 - acc: 0.9981 - val_loss: 0.0634 - val_acc: 0.9746\nEpoch 7/10\n48079/48079 [==============================] - 10s 199us/sample - loss: 0.0091 - acc: 0.9993 - val_loss: 0.0638 - val_acc: 0.9760\nEpoch 8/10\n48079/48079 [==============================] - 10s 200us/sample - loss: 0.0062 - acc: 0.9997 - val_loss: 0.0923 - val_acc: 0.9663\nEpoch 9/10\n48079/48079 [==============================] - 10s 202us/sample - loss: 0.0133 - acc: 0.9969 - val_loss: 0.1091 - val_acc: 0.9621\nEpoch 10/10\n48079/48079 [==============================] - 10s 207us/sample - loss: 0.0676 - acc: 0.9749 - val_loss: 0.1676 - val_acc: 0.9374\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"After parameter tuning I determined how many neurons should be in each layer and how many layers I need."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}